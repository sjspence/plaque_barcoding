{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plaque Barcoding pipeline\n",
    "Follow along our analysis steps and reproduce our results with the scripts below.  \n",
    "\n",
    "**Dependencies**\n",
    "* epicBarcoder (custom library on github)\n",
    "* pandas\n",
    "* pear\n",
    "* usearch v9.2\n",
    "* sina v1.2.11\n",
    "* fasttree  \n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import epicBarcoder as eb\n",
    "from scipy.stats import poisson\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up personal computing environment with paths to necessary tools and data directories\n",
    "env = os.environ\n",
    "dataDir = '/home/ubuntu/users/sjspence/170214_OM8/04_jupyter/'\n",
    "pearPath = '/usr/local/bin/pear'\n",
    "usearchPath = '/home/ubuntu/users/sjspence/tools/usearch9.2.64_i86linux32'\n",
    "sinaPath = '/home/ubuntu/bin/sina-1.2.11/sina'\n",
    "fasttreePath = '/home/ubuntu/bin/FastTree_dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import and edit mapping file\n",
    "sampIDs = []\n",
    "mapping = {}\n",
    "readCounts = {}\n",
    "with open(dataDir + 'OM8_map.txt', 'r') as inFile:\n",
    "    for line in inFile:\n",
    "        if '#' not in line:\n",
    "            line = line.strip().split('\\t')\n",
    "            mapping[line[1]] = line[0].replace('_','s')\n",
    "            readCounts[line[1]] = 0\n",
    "            sampIDs.append(line[0].replace('_','s'))\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join paired-end reads\n",
    "subprocess.call([pearPath, '-f', dataDir + '170214Alm_D17-2046_1_sequence.fastq', \n",
    "                 '-r', dataDir + '170214Alm_D17-2046_2_sequence.fastq', '-o', dataDir + '01_pear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71812424\n"
     ]
    }
   ],
   "source": [
    "#Break up file into pieces that usearch can use (5 million lines each)\n",
    "#Run this to completion before running next section\n",
    "inFile = open(dataDir + '01_pear.assembled.fastq', 'r')\n",
    "if not os.path.exists(dataDir + '02_pearSplits/'):\n",
    "    os.makedirs(dataDir + '02_pearSplits/')\n",
    "i = 0\n",
    "j = 1\n",
    "partFile = open(dataDir + '02_pearSplits/pear_' + str(j) + '.fastq', 'w')\n",
    "for line in inFile:\n",
    "    if i >= j*5000000:\n",
    "        partFile.close()\n",
    "        j += 1\n",
    "        partFile = open(dataDir + '02_pearSplits/pear_' + str(j) + '.fastq', 'w')\n",
    "    partFile.write(line)\n",
    "    i += 1\n",
    "partFile.close()\n",
    "inFile.close()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quality filter with usearch 9 max-error rate\n",
    "def qualFilter(inFile, outFile):\n",
    "    subprocess.call([usearchPath, \"-fastq_filter\", inFile, \"-fastq_minlen\", '100', '-fastq_maxee_rate', '0.01',\n",
    "                     \"-fastqout\", outFile], env=env)\n",
    "for filename in os.listdir(dataDir + '02_pearSplits/'):\n",
    "    qualFilter(dataDir + '02_pearSplits/' + filename, dataDir + '02_pearSplits/' + filename.replace('.fastq','filt.fastq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join quality-filtered files back into a single file for processing\n",
    "with open(dataDir + '02_pear_filt.fastq', 'w') as outfile:\n",
    "    for fname in os.listdir(dataDir + '02_pearSplits/'):\n",
    "        if 'filt' in fname:\n",
    "            with open(dataDir + '02_pearSplits/' + fname, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "            infile.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Demultiplex: check for barcodes and relabel sequences\n",
    "#Use mapping file to keep barcoded sequences, prepare fasta file\n",
    "with open(dataDir + '02_pear_filt.fastq', 'r') as inFile:\n",
    "    with open(dataDir + '03_pear_filt.fasta', 'w') as outFile:\n",
    "        i = 0\n",
    "        j = 0\n",
    "        nextSeq = False\n",
    "        for line in inFile:\n",
    "            if nextSeq:\n",
    "                outFile.write(line)\n",
    "                nextSeq = False\n",
    "            if i%4 == 0:\n",
    "                for bc in mapping:\n",
    "                    if bc in line:\n",
    "                        readCounts[bc] += 1\n",
    "                        newLine = line.strip().replace('@','>' + mapping[bc] + '_' + str(j) + ' ')\n",
    "                        newLine = newLine + ' orig_bc=' + bc + ' new_bc=' + bc + ' bc_diffs=0\\n'\n",
    "                        outFile.write(newLine)\n",
    "                        nextSeq = True\n",
    "                        j += 1\n",
    "            i += 1\n",
    "inFile.close()\n",
    "outFile.close()\n",
    "#Summarize read mapping after quality filtering and zero-error barcode matching\n",
    "total = 0\n",
    "summaryFile = open(dataDir + '03_quality_summary.txt', 'w')\n",
    "for s in sampIDs:\n",
    "    for bc in mapping:\n",
    "        if mapping[bc] == s:\n",
    "            summaryFile.write(s + '\\t' + str(readCounts[bc]) + '\\n')\n",
    "            total += readCounts[bc]\n",
    "summaryFile.write('Total\\t' + str(total))\n",
    "summaryFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Primer check and removal, placing droplet barcode into header\n",
    "#NOTE: this takes a while\n",
    "qualReads = eb.importFasta(dataDir + '03_pear_filt.fasta')\n",
    "noPrimerReads = eb.filtBarcodePrimers(qualReads, 20, 'GATCATGACCCATTTGGAGAAGATG', 'GGACTACHVGGGTWTCTAAT')\n",
    "eb.exportFasta(noPrimerReads, dataDir + '04_pear_noPrimers.fasta')\n",
    "print(len(qualReads))\n",
    "print(len(noPrimerReads))\n",
    "print(noPrimerReads[0].header)\n",
    "print(noPrimerReads[0].seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dereplication and denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collapse identical reads and maintain the provenance to reduce the uclust file size\n",
    "#uniqueDict maps a unique sequence to a list of read objects which contain it\n",
    "#NOTE: takes a long time, but rerun after notebook closes out\n",
    "#OUTPUT: uniqueDict is a dictionary mapping a unique sequence to a list of read objects which contain it\n",
    "uniqueDict = eb.getUniqueSeqs(dataDir + '04_pear_noPrimers.fasta', dataDir + '05_unique_seqs.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the usearch unoise algorithm to create zero radius OTUs (zOTUs), while also discarding chimeras, phiX sequences,\n",
    "#and low complexity DNA\n",
    "#Input: unique sequences collapsed from quality- and primer- filtered data\n",
    "#Output: Denoised file with true biological reads\n",
    "#        Database file with true amplicon reads including chimeras\n",
    "subprocess.call([usearchPath, '-unoise2', dataDir + '05_unique_seqs.fasta', '-fastaout', dataDir + '06_denoised.fa',\n",
    "                 '-otudbout', dataDir + '06_db.fa', '-minampsize', '3'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Unoise output**  \n",
    "00:02 332Mb   100.0% Reading 05_unique_seqs.fasta  \n",
    "00:04 352Mb   100.0% 3955 amplicons, 1791728 bad (size >= 3)\n",
    "01:55 359Mb   100.0% 354 good, 3601 chimeras\n",
    "\n",
    "46735 corrected amplicon sequences (including chimeras) in 06_db.fa  \n",
    "354 output biological sequences in 06_denoised.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine taxonomic information to export final file with droplet barcodes\n",
    "#Added an if statement, since this variable technically gets created earlier in the pipeline\n",
    "if 'uniqueDict' not in globals():\n",
    "    uniqueDict = eb.getUniqueSeqs(dataDir + '04_pear_noPrimers.fasta', dataDir + '05_unique_seqs.fasta')\n",
    "\n",
    "#Take denoised zOTUs, then map back to original reads and rewrite original read file (minus noisy reads) with\n",
    "#zOTU and information in the headers\n",
    "eb.expandDenoised(uniqueDict, dataDir + '06_denoised.fa', dataDir + '08_denoised_all_seqs.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read loss from unoise2**\n",
    "\n",
    "24490106 04_pear_noPrimers.fasta = 12,245,053 reads prior to unoise2  \n",
    "18228922 08_all_seqs_tax.fa = 9,114,461 reads after unoise2  \n",
    "\n",
    "Approximately 25% read loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: ONLY NEED TO RUN THIS ONCE\n",
    "\n",
    "#Format fasta database for input to SINTAX\n",
    "#Maintained HOMD HOT strain ID in header following the taxonomic information\n",
    "#Example SINTAX header structure below:\n",
    "#>AB008314;tax=d:Bacteria,p:Firmicutes,c:Bacilli,o:Lactobacillales,f:Streptococcaceae,g:Streptococcus;\n",
    "outFile = open(dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.fasta', 'w')\n",
    "taxDict = {}\n",
    "with open(dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.qiime_spike.taxonomy', 'r') as t:\n",
    "    for line in t:\n",
    "        line = line.strip().split('\\t')\n",
    "        taxID = line[0]\n",
    "        tax = line[1].strip().replace('__',':')\n",
    "        tax = tax.replace(';',',')\n",
    "        taxDict[taxID] = tax\n",
    "with open(dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_spike.fasta', 'r') as f:\n",
    "    for line in f:\n",
    "        if '>' in line:\n",
    "            line = line.strip().split(' ')\n",
    "            taxInfo = taxDict[line[0].replace('>','')]\n",
    "            outLine = line[0] + ';tax=' + taxInfo + ';'\n",
    "            for i in line:\n",
    "                if 'HOT' in i:\n",
    "                    outLine += i + ';'\n",
    "            outFile.write(outLine + '\\n')\n",
    "        else:\n",
    "            outFile.write(line)\n",
    "outFile.close()\n",
    "subprocess.call([usearchPath, '-makeudb_sintax', dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.fasta', \n",
    "                 '-output', dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.udb'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Database formatting output**  \n",
    "00:00 14Mb   1020 names, tax levels min 7, avg 7.0, max 7  \n",
    "WARNING: 25 taxonomy nodes have >1 parent  \n",
    "00:00 14Mb   Buffers (892 seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run SINTAX to determine denoised read taxonomic information\n",
    "#Default is to run one thread per CPU core, or 10 threads if there are > 10 cores\n",
    "subprocess.call([usearchPath, '-sintax', dataDir + '06_denoised.fa', \n",
    "                 '-db', dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.udb', \n",
    "                 '-tabbedout', dataDir + '07_denoised.sintax', \n",
    "                 '-strand', 'plus', '-sintax_cutoff', '0.8', '-threads', '4'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Droplet barcode parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Within each sample, group by barcode; quantify unique barcode pairings\n",
    "#Input: Fasta file with droplet barcode, otu, and taxonomic information in the header\n",
    "#Output: A dictionary where each sampID maps to a dictionary of droplet barcodes:[zOTU1, zOTU1, zOTU2, ...]\n",
    "barcodeDict = eb.createBarcodeDict(dataDir + '08_denoised_all_seqs.fa')\n",
    "\n",
    "#Export summary file of droplet barcodes per sample\n",
    "if not os.path.exists(dataDir + '08_barcoding_log.txt'):\n",
    "    eb.summarizeBarcoding(barcodeDict, sampIDs, dataDir + '08_barcoding_log.txt')\n",
    "\n",
    "#Import sintax taxonomy as a dictionary mapping zOTU IDs to a >80% taxonomic assignment string\n",
    "taxDict = eb.importSintax(dataDir + '07_denoised.sintax', 'final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create pandas dataframe with the relative abundances of different tOTUs calculated from singleton barcode data\n",
    "abundanceDf = eb.tOTU_singletonAbundances(barcodeDict, taxDict)\n",
    "\n",
    "#Create pandas data frame with tOTU pairs in rows (as 'tOTU1__tOTU2') and sample IDs in columns.  Data is the number\n",
    "#of barcodes supporting the pair\n",
    "pairDf = eb.tOTU_quantifyPairs(barcodeDict, taxDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shewanella tOTU: tOtu10\n"
     ]
    }
   ],
   "source": [
    "#Identify shewanella oneidensis tOTU to feed into connection files for significance thresholding\n",
    "shew_tOTU = ''\n",
    "otuDf = eb.tOTUmap(taxDict)\n",
    "for i, tax in enumerate(otuDf['taxonomy']):\n",
    "    if 'g:Shewanella,s:oneidensis' in tax:\n",
    "        shew_tOTU = otuDf['tOTU'].iloc[i]\n",
    "        break\n",
    "print('Shewanella tOTU: ' + shew_tOTU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep iTol files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make tree for tOTUs in the complete dataset\n",
    "#Choose representative sequence based on abundance in taxonomic group, export fasta of representative seqs\n",
    "eb.tOTU_pickRepSeqs(dataDir + '06_denoised.fa', dataDir + '07_denoised.sintax', dataDir + '09_repSeqs.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Align and build tree from tOTUs\n",
    "#Produce alignments with the representative OTU sequences, use the silva aligner against reference\n",
    "inFile = dataDir + '09_repSeqs.fa'\n",
    "outFile = inFile.replace('.fa', '_aligned.fa')\n",
    "database = dataDir + 'SSURef_NR99_128_SILVA_07_09_16_opt.arb'\n",
    "logFile = open(inFile.replace('.fa', '_sinaLog.txt'), 'w')\n",
    "subprocess.call([sinaPath, \"-i\", inFile, \"-o\", outFile, \"--intype\", 'fasta', \"--outtype\",\n",
    "                 'fasta', \"--ptdb\", database], stdout=logFile, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the tree\n",
    "inFile = dataDir + '09_repSeqs_aligned.fa'\n",
    "outFile = open(inFile.replace('_aligned.fa', '.tre'), 'w')\n",
    "subprocess.call([fasttreePath, '-nt', '-gtr', inFile], stdout=outFile, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make iTol abundance files and zip them into an output directory\n",
    "eb.itolSimpleBar(abundanceDf, dataDir + '09_itol_abundances/')\n",
    "\n",
    "#TODO\n",
    "##change max width to 100\n",
    "##change bar colors\n",
    "##add outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make iTol total connection files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make iTol significant connection files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make iTol file with taxonomic hover-over info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate poisson probabilities that two bugs would co-occur and filter results based on that\n",
    "#Print tab-delimited format showing total #pairs, #significant pairs, #shew to other, #shew to self\n",
    "#Create iTol file with significant connections\n",
    "#Also make iTol file with relative abundances...?\n",
    "def makeConnectionFile(sampID, direction, cutoff, pairs, relAbundances, totals, shew_tOTU):\n",
    "    shewYes = 0\n",
    "    shewNo = 0\n",
    "    i = 0\n",
    "    sigPairs_pos = {}\n",
    "    sigPairs_neg = {}\n",
    "    for otuPair in pairs:\n",
    "        otu1 = otuPair.split('__')[0]\n",
    "        otu2 = otuPair.split('__')[1]\n",
    "        if otu1 in relAbundances[sampID]:\n",
    "            a1 = relAbundances[sampID][otu1]\n",
    "        else:\n",
    "            a1 = 0.0\n",
    "        if otu2 in relAbundances[sampID]:\n",
    "            a2 = relAbundances[sampID][otu2]\n",
    "        else:\n",
    "            a2 = 0.0\n",
    "        x = pairs[otuPair]\n",
    "        mu = a1 * a2 * totals[sampID]\n",
    "        p = poisson.pmf(x, mu)\n",
    "        if p < cutoff:\n",
    "            i += 1\n",
    "            if x < mu:\n",
    "                sigPairs_neg[otuPair] = pairs[otuPair]\n",
    "            else:\n",
    "                sigPairs_pos[otuPair] = pairs[otuPair]\n",
    "            if (otu1 == shew_tOTU) or (otu2 == shew_tOTU):\n",
    "                if x < mu:\n",
    "                    shewYes += 1\n",
    "                else:\n",
    "                    shewNo += 1\n",
    "    if (shewNo == 0) and (i >= 1):\n",
    "        if not os.path.exists(dataDir + '09_itol_connections/'):\n",
    "            os.makedirs(dataDir + '09_itol_connections/')\n",
    "        connectFile = open(dataDir + '09_itol_connections/' + sampID + '_' + direction + '.txt', 'w')\n",
    "        connectFile.write('DATASET_CONNECTION\\n')\n",
    "        connectFile.write('SEPARATOR COMMA\\n')\n",
    "        connectFile.write('DATASET_LABEL,' + sampID + '_' + direction + '\\n')\n",
    "        if direction == 'pos':\n",
    "            connectFile.write('COLOR,#ff0000\\n')\n",
    "        else:\n",
    "            connectFile.write('COLOR,#0000ff\\n')\n",
    "        connectFile.write('DRAW_ARROWS,0\\n')\n",
    "        connectFile.write('MAXIMUM_LINE_WIDTH,20\\n')\n",
    "        connectFile.write('CENTER_CURVES,1\\n')\n",
    "        connectFile.write('DATA\\n')\n",
    "        #NODE1,NODE2,WIDTH,COLOR,LABEL\n",
    "        if direction == 'pos':\n",
    "            for p in sigPairs_pos:\n",
    "                connectFile.write(p.split('__')[0] + ',' + p.split('__')[1] + ',' + str(sigPairs_pos[p]) + ',' + \\\n",
    "                             '#ff0000\\n')\n",
    "        else:\n",
    "            for p in sigPairs_neg:\n",
    "                connectFile.write(p.split('__')[0] + ',' + p.split('__')[1] + ',' + str(sigPairs_neg[p]) + ',' + \\\n",
    "                             '#0000ff\\n')\n",
    "        connectFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create positive and negative iTol files for samples which look clean based on spike-in\n",
    "for s in sampIDs:\n",
    "    if s not in pairDict:\n",
    "        continue\n",
    "    cutoff = 1e-5\n",
    "    makeConnectionFile(s, 'pos', cutoff, pairDict[s], relAbundances, totals, shew_tOTU)\n",
    "    makeConnectionFile(s, 'neg', cutoff, pairDict[s], relAbundances, totals, shew_tOTU)\n",
    "\n",
    "os.system('zip -r ' + dataDir + '09_itol_connections.zip ' + dataDir + '09_itol_connections/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OM8s01\t402\t80\t3\t15\n",
      "OM8s02\t437\t87\t24\t1\n",
      "OM8s03\t338\t74\t16\t1\n",
      "OM8s04\t466\t69\t16\t5\n",
      "OM8s05\t818\t148\t21\t6\n",
      "OM8s06\t873\t201\t20\t15\n",
      "OM8s07\t293\t191\t45\t0\n",
      "OM8s08\t593\t160\t5\t19\n",
      "OM8s09\t156\t13\t2\t0\n",
      "OM8s10\t155\t75\t23\t0\n",
      "OM8s11\t813\t234\t55\t0\n",
      "OM8s12\t507\t155\t35\t0\n",
      "OM8s13\t9\t3\t1\t0\n",
      "OM8s14\t3\t1\t0\t1\n",
      "OM8s15\t2\t0\t0\t0\n",
      "OM8s16\t2\t0\t0\t0\n",
      "OM8s17\t4\t0\t0\t0\n",
      "OM8s18\t2\t0\t0\t0\n",
      "OM8s19\t0\t0\t0\t0\n",
      "OM8s20\t0\t0\t0\t0\n",
      "OM8s21\t987\t443\t8\t36\n",
      "OM8s22\t211\t47\t11\t0\n",
      "OM8s23\t381\t66\t13\t7\n",
      "OM8s24\t549\t103\t14\t10\n",
      "OM8s25\t209\t34\t2\t0\n",
      "OM8s26\t277\t70\t7\t0\n",
      "OM8s27\t0\t0\t0\t0\n",
      "OM8s28\t2\t1\t1\t0\n",
      "OM8s29\t0\t0\t0\t0\n",
      "OM8s30\t0\t0\t0\t0\n",
      "OM8s31\t0\t0\t0\t0\n",
      "OM8s32\t0\t0\t0\t0\n",
      "OM8s34\t0\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "#Calculate poisson probabilities that two bugs would co-occur and filter results based on that\n",
    "#Print tab-delimited format showing total #pairs, #significant pairs, #shew to other, #shew to self\n",
    "#Create iTol file with significant connections\n",
    "cutoff = 1e-5\n",
    "for s in sampIDs:\n",
    "    if s not in pairDict:\n",
    "        continue\n",
    "    i = 0\n",
    "    t = 0\n",
    "    shewYes = 0\n",
    "    shewNo = 0\n",
    "    doubleShew = 0\n",
    "    for otuPair in pairDict[s]:\n",
    "        t += 1\n",
    "        otu1 = otuPair.split('__')[0]\n",
    "        otu2 = otuPair.split('__')[1]\n",
    "        if otu1 in relAbundances[s]:\n",
    "            a1 = relAbundances[s][otu1]\n",
    "        else:\n",
    "            a1 = 0.0\n",
    "        if otu2 in relAbundances[s]:\n",
    "            a2 = relAbundances[s][otu2]\n",
    "        else:\n",
    "            a2 = 0.0\n",
    "        x = pairDict[s][otuPair]\n",
    "        mu = a1 * a2 * totals[s]\n",
    "        p = poisson.pmf(x, mu)\n",
    "        if p < cutoff:\n",
    "            i += 1\n",
    "            if ('oneidensis' in tOTU_tax[otu1]) and ('oneidensis' in tOTU_tax[otu2]):\n",
    "                doubleShew += 1\n",
    "            elif ('oneidensis' in tOTU_tax[otu1]) or ('oneidensis' in tOTU_tax[otu2]):\n",
    "                if x < mu:\n",
    "                    shewYes += 1\n",
    "                else:\n",
    "                    shewNo += 1\n",
    "    print(s + '\\t' + str(t) + '\\t' + str(i) + '\\t' + str(shewYes) + '\\t' + str(shewNo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OM8s01\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.110946487711\n",
      "\n",
      "OM8s02\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.420078990954\n",
      "\n",
      "OM8s03\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.233302806306\n",
      "\n",
      "OM8s04\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.342921615202\n",
      "\n",
      "OM8s05\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.581176644822\n",
      "\n",
      "OM8s06\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.536212835388\n",
      "\n",
      "OM8s07\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.222974646957\n",
      "\n",
      "OM8s08\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.27106545961\n",
      "\n",
      "OM8s09\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.0158058771149\n",
      "\n",
      "OM8s10\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.240303785727\n",
      "\n",
      "OM8s11\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.122015794038\n",
      "\n",
      "OM8s12\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.102167182663\n",
      "\n",
      "OM8s13\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.991327016594\n",
      "\n",
      "OM8s14\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.997021118145\n",
      "\n",
      "OM8s15\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.990822881615\n",
      "\n",
      "OM8s16\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.994499927631\n",
      "\n",
      "OM8s17\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.997379126897\n",
      "\n",
      "OM8s18\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.997123158573\n",
      "\n",
      "OM8s19\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.1875\n",
      "\n",
      "OM8s20\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.333333333333\n",
      "\n",
      "OM8s21\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.104605129329\n",
      "\n",
      "OM8s22\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.185478852957\n",
      "\n",
      "OM8s23\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.231646185055\n",
      "\n",
      "OM8s24\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.332459891726\n",
      "\n",
      "OM8s25\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.00834524269009\n",
      "\n",
      "OM8s26\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.0558557622363\n",
      "\n",
      "OM8s27\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.5\n",
      "\n",
      "OM8s28\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.170301783265\n",
      "\n",
      "OM8s29\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.5\n",
      "\n",
      "OM8s30\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.74358974359\n",
      "\n",
      "OM8s31\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "0.6\n",
      "\n",
      "OM8s32\n",
      "OM8s34\n",
      "k:Bacteria,p:Proteobacteria,c:Gammaproteobacteria,o:Alteromonadales,f:Shewanellaceae,g:Shewanella,s:oneidensis\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What are the s. oneidensis relative abundances?\n",
    "for s in sampIDs:\n",
    "    if s not in relAbundances:\n",
    "        continue\n",
    "    print(s)\n",
    "    for otu in relAbundances[s]:\n",
    "        if 'oneidensis' in otu:\n",
    "            print(otu + '\\n' + str(relAbundances[s][otu]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try cluster_fast clustering\n",
    "subprocess.call([usearchPath, '-cluster_fast', dataDir + '06_denoised.fa', '-id', '0.97', '-centroids', \n",
    "                dataDir + '09_otu_clusters.fa', '-uc', dataDir + '09_otu_clusters.uc'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Seqs  354  \n",
    "  Clusters  154  \n",
    "  Max size  17  \n",
    "  Avg size  2.3  \n",
    "  Min size  1  \n",
    "Singletons  74, 20.9% of seqs, 48.1% of clusters  \n",
    "   Max mem  83Mb  \n",
    "      Time  1.00s  \n",
    "Throughput  354.0 seqs/sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import list of read objects from unoise2 denoised file\n",
    "denoised = eb.importFasta(dataDir + '06_denoised.fa')\n",
    "\n",
    "#Import Otu header:[tax probabilities, taxonomy] dictionary from SINTAX output\n",
    "taxDict = eb.importSintax(dataDir + '07_denoised.sintax')\n",
    "\n",
    "#Import hits from 97% fast clustering\n",
    "hits = eb.importClusterFast(dataDir + '09_otu_clusters.uc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otu6;uniq=OM8s21_7\n",
      "['k:Bacteria(1.0000),p:Fusobacteria(1.0000),c:Fusobacteriia(1.0000),o:Fusobacteriales(1.0000),f:Fusobacteriaceae(1.0000),g:Fusobacterium(1.0000),s:nucleatum_subsp._polymorphum(0.7700)', 'k:Bacteria,p:Fusobacteria,c:Fusobacteriia,o:Fusobacteriales,f:Fusobacteriaceae,g:Fusobacterium']\n"
     ]
    }
   ],
   "source": [
    "#FIRST TRY TO SEE HOW MUCH SHEWANELLA COLLAPSES BY TAXONOMY\n",
    "i = 0\n",
    "for t in taxDict:\n",
    "    if i == 0:\n",
    "        print(t)\n",
    "        print(taxDict[t])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otu111;uniq=OM8s08_7689 HWI-M04407:1:2105:24185:5342#TCTGTATG/1 orig_bc=TCTGTATG new_bc=TCTGTATG bc_diffs=0 droplet_bc=TTTGCCTTGAGCAGAGAACA;size=1956;\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Check number of shewanella seqs in 97% otu clusters\n",
    "i = 0\n",
    "j = 0\n",
    "shewSeqs = []\n",
    "for h in hits:\n",
    "    if j == 0:\n",
    "        print(h)\n",
    "    j += 1\n",
    "    seqID = h.split(' ')[0]\n",
    "    if 'oneidensis' in taxDict[seqID][1]:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "shewReads = []\n",
    "for read in denoised:\n",
    "    if (read.header.replace('>','') in hits) and ('Shew' in taxDict[read.seq_id][1]):\n",
    "        shewReads.append(read)\n",
    "print(len(shewReads))\n",
    "eb.exportFasta(shewReads, dataDir + 'multiple_shew_97_otus.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "10\n",
      "5\n",
      "4\n",
      "7\n",
      "88\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "#How many taxonomic indications map all the way to species?  Can I make a taxonomy OTU table?\n",
    "total = 0\n",
    "phylum = 0\n",
    "clas = 0\n",
    "order = 0\n",
    "family = 0\n",
    "genus = 0\n",
    "species = 0\n",
    "for t in taxDict:\n",
    "    total += 1\n",
    "    if 's:' in taxDict[t][1]:\n",
    "        species +=1\n",
    "    elif 'g:' in taxDict[t][1]:\n",
    "        genus += 1\n",
    "    elif 'f:' in taxDict[t][1]:\n",
    "        family += 1\n",
    "    elif 'o:' in taxDict[t][1]:\n",
    "        order += 1\n",
    "    elif 'c:' in taxDict[t][1]:\n",
    "        clas += 1\n",
    "    elif 'p:' in taxDict[t][1]:\n",
    "        phylum += 1\n",
    "print(total)\n",
    "print(phylum)\n",
    "print(clas)\n",
    "print(order)\n",
    "print(family)\n",
    "print(genus)\n",
    "print(species)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
