{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import epicBarcoder as eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = os.environ\n",
    "dataDir = '/home/ubuntu/users/sjspence/170214_OM8/03_jupyter/'\n",
    "pearPath = '/usr/local/bin/pear'\n",
    "#usearchPath = '/home/ubuntu/bin/usearch8'\n",
    "usearchPath = '/home/ubuntu/users/sjspence/tools/usearch9.2.64_i86linux32'\n",
    "sinaPath = '/home/ubuntu/bin/sina-1.2.11/sina'\n",
    "fasttreePath = '/home/ubuntu/bin/FastTree_dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join paired-end reads\n",
    "subprocess.call([pearPath, '-f', dataDir + '170214Alm_D17-2046_1_sequence.fastq', \n",
    "                 '-r', dataDir + '170214Alm_D17-2046_2_sequence.fastq', '-o', dataDir + '01_pear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71812424\n"
     ]
    }
   ],
   "source": [
    "#Break up file into pieces that usearch can use (5 million lines each)\n",
    "#Run this to completion before running next section\n",
    "inFile = open(dataDir + '01_pear.assembled.fastq', 'r')\n",
    "if not os.path.exists(dataDir + '02_pearSplits/'):\n",
    "    os.makedirs(dataDir + '02_pearSplits/')\n",
    "i = 0\n",
    "j = 1\n",
    "partFile = open(dataDir + '02_pearSplits/pear_' + str(j) + '.fastq', 'w')\n",
    "for line in inFile:\n",
    "    if i >= j*5000000:\n",
    "        partFile.close()\n",
    "        j += 1\n",
    "        partFile = open(dataDir + '02_pearSplits/pear_' + str(j) + '.fastq', 'w')\n",
    "    partFile.write(line)\n",
    "    i += 1\n",
    "partFile.close()\n",
    "inFile.close()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quality filter with usearch 9 max-error rate\n",
    "def qualFilter(inFile, outFile):\n",
    "    subprocess.call([usearchPath, \"-fastq_filter\", inFile, \"-fastq_minlen\", '100', '-fastq_maxee_rate', '0.01',\n",
    "                     \"-fastqout\", outFile], env=env)\n",
    "for filename in os.listdir(dataDir + '02_pearSplits/'):\n",
    "    qualFilter(dataDir + '02_pearSplits/' + filename, dataDir + '02_pearSplits/' + filename.replace('.fastq','filt.fastq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join quality-filtered files back into a single file for processing\n",
    "with open(dataDir + '02_pear_filt.fastq', 'w') as outfile:\n",
    "    for fname in os.listdir(dataDir + '02_pearSplits/'):\n",
    "        if 'filt' in fname:\n",
    "            with open(dataDir + '02_pearSplits/' + fname, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "            infile.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import and edit mapping file\n",
    "sampIDs = []\n",
    "mapping = {}\n",
    "readCounts = {}\n",
    "with open(dataDir + 'OM8_map.txt', 'r') as inFile:\n",
    "    for line in inFile:\n",
    "        if '#' not in line:\n",
    "            line = line.strip().split('\\t')\n",
    "            mapping[line[1]] = line[0].replace('_','s')\n",
    "            readCounts[line[1]] = 0\n",
    "            sampIDs.append(line[0].replace('_','s'))\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Demultiplex: check for barcodes and relabel sequences\n",
    "#Use mapping file to keep barcoded sequences, prepare fasta file\n",
    "with open(dataDir + '02_pear_filt.fastq', 'r') as inFile:\n",
    "    with open(dataDir + '03_pear_filt.fasta', 'w') as outFile:\n",
    "        i = 0\n",
    "        j = 0\n",
    "        nextSeq = False\n",
    "        for line in inFile:\n",
    "            if nextSeq:\n",
    "                outFile.write(line)\n",
    "                nextSeq = False\n",
    "            if i%4 == 0:\n",
    "                for bc in mapping:\n",
    "                    if bc in line:\n",
    "                        readCounts[bc] += 1\n",
    "                        newLine = line.strip().replace('@','>' + mapping[bc] + '_' + str(j) + ' ')\n",
    "                        newLine = newLine + ' orig_bc=' + bc + ' new_bc=' + bc + ' bc_diffs=0\\n'\n",
    "                        outFile.write(newLine)\n",
    "                        nextSeq = True\n",
    "                        j += 1\n",
    "            i += 1\n",
    "inFile.close()\n",
    "outFile.close()\n",
    "#Summarize read mapping after quality filtering and zero-error barcode matching\n",
    "total = 0\n",
    "summaryFile = open(dataDir + '03_quality_summary.txt', 'w')\n",
    "for s in sampIDs:\n",
    "    for bc in mapping:\n",
    "        if mapping[bc] == s:\n",
    "            summaryFile.write(s + '\\t' + str(readCounts[bc]) + '\\n')\n",
    "            total += readCounts[bc]\n",
    "summaryFile.write('Total\\t' + str(total))\n",
    "summaryFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Primer check and removal, placing droplet barcode into header\n",
    "#NOTE: this takes a while\n",
    "qualReads = eb.importFasta(dataDir + '03_pear_filt.fasta')\n",
    "noPrimerReads = eb.filtBarcodePrimers(qualReads, 20, 'GATCATGACCCATTTGGAGAAGATG', 'GGACTACHVGGGTWTCTAAT')\n",
    "eb.exportFasta(noPrimerReads, dataDir + '04_pear_noPrimers.fasta')\n",
    "print(len(qualReads))\n",
    "print(len(noPrimerReads))\n",
    "print(noPrimerReads[0].header)\n",
    "print(noPrimerReads[0].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collapse identical reads and maintain the provenance to reduce the uclust file size\n",
    "#uniqueDict maps a unique sequence to a list of read objects which contain it\n",
    "#NOTE: takes a long time, but rerun after notebook closes out\n",
    "noPrimerReads = eb.importFasta(dataDir + '04_pear_noPrimers.fasta')\n",
    "uniqueDict = eb.getUniqueSeqs(noPrimerReads, dataDir + '05_unique_seqs.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the usearch unoise algorithm to create zero radius OTUs (zOTUs), while also discarding chimeras, phiX sequences,\n",
    "#and low complexity DNA\n",
    "#Input: unique sequences collapsed from quality- and primer- filtered data\n",
    "#Output: Denoised file with true biological reads\n",
    "#        Database file with true amplicon reads including chimeras\n",
    "subprocess.call([usearchPath, '-unoise2', dataDir + '05_unique_seqs.fasta', '-fastaout', dataDir + '06_denoised.fa',\n",
    "                 '-otudbout', dataDir + '06_db.fa', '-minampsize', '1'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Unoise output**  \n",
    "00:02 332Mb   100.0% Reading 05_unique_seqs.fasta  \n",
    "01:04 637Mb   100.0% 27163 amplicons, 2096896 bad (size >= 1)  \n",
    "46:06 650Mb   100.0% 14693 good, 12470 chimeras\n",
    "\n",
    "245334 corrected amplicon sequences (including chimeras) in 06_db.fa  \n",
    "14693 output biological sequences in 06_denoised.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Format fasta database for input to SINTAX\n",
    "#>AB008314;tax=d:Bacteria,p:Firmicutes,c:Bacilli,o:Lactobacillales,f:Streptococcaceae,g:Streptococcus;\n",
    "#Maintained HOMD HOT strain ID in header following the taxonomic information\n",
    "outFile = open(dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.fasta', 'w')\n",
    "taxDict = {}\n",
    "with open(dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.qiime_spike.taxonomy', 'r') as t:\n",
    "    for line in t:\n",
    "        line = line.strip().split('\\t')\n",
    "        taxID = line[0]\n",
    "        tax = line[1].strip().replace('__',':')\n",
    "        tax = tax.replace(';',',')\n",
    "        taxDict[taxID] = tax\n",
    "with open(dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_spike.fasta', 'r') as f:\n",
    "    for line in f:\n",
    "        if '>' in line:\n",
    "            line = line.strip().split(' ')\n",
    "            taxInfo = taxDict[line[0].replace('>','')]\n",
    "            outLine = line[0] + ';tax=' + taxInfo + ';'\n",
    "            for i in line:\n",
    "                if 'HOT' in i:\n",
    "                    outLine += i + ';'\n",
    "            outFile.write(outLine + '\\n')\n",
    "        else:\n",
    "            outFile.write(line)\n",
    "outFile.close()\n",
    "subprocess.call([usearchPath, '-makeudb_sintax', dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.fasta', \n",
    "                 '-output', dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.udb'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Database formatting output**  \n",
    "00:00 14Mb   1020 names, tax levels min 7, avg 7.0, max 7  \n",
    "WARNING: 25 taxonomy nodes have >1 parent  \n",
    "00:00 14Mb   Buffers (892 seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run SINTAX to determine denoised read taxonomic information\n",
    "#Default is to run one thread per CPU core, or 10 threads if there are > 10 cores\n",
    "subprocess.call([usearchPath, '-sintax', dataDir + '06_denoised.fa', \n",
    "                 '-db', dataDir + 'HOMD_16S_rRNA_RefSeq_V14.5.p9_sintax_spike.udb', \n",
    "                 '-tabbedout', dataDir + '07_denoised.sintax', \n",
    "                 '-strand', 'plus', '-sintax_cutoff', '0.8', '-threads', '4'], env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Combine taxonomic information to export final file with droplet barcodes and taxonomies\n",
    "#06_denoised.fa: header matches the first tabbed column of sintax output (minus '>'), sequence follows\n",
    "#Import list of read objects from unoise2 denoised file\n",
    "denoised = eb.importFasta(dataDir + '06_denoised.fa')\n",
    "\n",
    "#Import Otu header:[tax probabilities, taxonomy] dictionary from SINTAX output\n",
    "taxDict = eb.importSintax(dataDir + '07_denoised.sintax')\n",
    "\n",
    "#Take denoised zOTUs and taxonomic information, then map back to original reads and rewrite original read file with\n",
    "#zOTU and taxonomic information in the headers\n",
    "eb.otuToHeaders(denoised, taxDict, uniqueDict, dataDir + '08_all_seqs_tax.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read loss from unoise2**\n",
    "\n",
    "24490106 04_pear_noPrimers.fasta = 12,245,053 reads prior to unoise2  \n",
    "18257786 08_allSeqsTax.fa = 9,128,893 reads after unoise2  \n",
    "\n",
    "Approximately 25% read loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Within each sample, group by barcode; quantify unique barcode pairings\n",
    "#Need a structure like [sampID1, sampID2, etc.]\n",
    "#Then each sampID maps to a dictionary of droplet barcodes:[otu1, otu2]\n",
    "def createBarcodeDict(inFileName):\n",
    "    inFile = open(inFileName, 'r')\n",
    "    barcodeSamples = {}\n",
    "    for line in inFile:\n",
    "        if '>' in line:\n",
    "            line = line.strip().split(';')\n",
    "            samp = line[0].split('_')[0].replace('>','')\n",
    "            bc = line[0].split('droplet_bc=')[1]\n",
    "            otu = line[1]\n",
    "            tax = line[2].replace('tax=','')\n",
    "            if samp not in barcodeSamples:\n",
    "                barcodeSamples[samp] = {bc:[[otu, tax]]}\n",
    "            else:\n",
    "                if bc not in barcodeSamples[samp]:\n",
    "                    barcodeSamples[samp][bc] = [[otu, tax]]\n",
    "                else:\n",
    "                    barcodeSamples[samp][bc].append([otu, tax])\n",
    "    inFile.close()\n",
    "    return barcodeSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "barcodeDict = createBarcodeDict(dataDir + '08_all_seqs_tax.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OM8s01\t31613\n",
      "OM8s02\t53727\n",
      "OM8s03\t39763\n",
      "OM8s04\t56228\n",
      "OM8s05\t42798\n",
      "OM8s06\t32556\n",
      "OM8s07\t41151\n",
      "OM8s08\t15048\n",
      "OM8s09\t26675\n",
      "OM8s10\t19429\n",
      "OM8s11\t49765\n",
      "OM8s12\t25560\n",
      "OM8s13\t24283\n",
      "OM8s14\t37869\n",
      "OM8s15\t5625\n",
      "OM8s16\t11355\n",
      "OM8s17\t69740\n",
      "OM8s18\t28834\n",
      "OM8s19\t40\n",
      "OM8s20\t83\n",
      "OM8s21\t221776\n",
      "OM8s22\t71263\n",
      "OM8s23\t85362\n",
      "OM8s24\t58431\n",
      "OM8s25\t145900\n",
      "OM8s26\t82931\n",
      "OM8s27\t7\n",
      "OM8s28\t29651\n",
      "OM8s29\t146\n",
      "OM8s30\t68\n",
      "OM8s31\t9\n",
      "OM8s32\t1\n",
      "OM8s34\t2\n"
     ]
    }
   ],
   "source": [
    "#Print number of barcodes per sample\n",
    "for s in sampIDs:\n",
    "    if s in barcodeDict:\n",
    "        sample = barcodeDict[s]\n",
    "        print(s + '\\t' + str(len(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print number of barcodes per sample\n",
    "pairDicts = {}\n",
    "for s in sampIDs:\n",
    "    if s in barcodeDict:\n",
    "        uniquePairs = {}\n",
    "        for bc in barcodeDict[s]:\n",
    "            if len(barcodeDict[s][bc]) != 1:\n",
    "                for otu in barcodeDict[s][bc]:\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quantify unique pairings\n",
    "pairDicts = {}\n",
    "for s in sampIDs:\n",
    "    if s in barcodeSamples:\n",
    "        uniquePairs = {}\n",
    "        for bc in barcodeSamples[s]:\n",
    "            if len(barcodeSamples[s][bc]) != 1:\n",
    "                uniqueOTUs = set(barcodeSamples[s][bc])\n",
    "                if len(uniqueOTUs) != 1:\n",
    "                    pairString = '_'.join(list(uniqueOTUs))\n",
    "                    if pairString not in uniquePairs:\n",
    "                        uniquePairs[pairString] = 1\n",
    "                    else:\n",
    "                        uniquePairs[pairString] += 1\n",
    "        pairDicts[s] = uniquePairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
