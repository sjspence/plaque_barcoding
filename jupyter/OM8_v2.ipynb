{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import epicBarcoder as eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = os.environ\n",
    "dataDir = '/home/ubuntu/users/sjspence/170214_OM8/03_jupyter/'\n",
    "pearPath = '/usr/local/bin/pear'\n",
    "#usearchPath = '/home/ubuntu/bin/usearch8'\n",
    "usearchPath = '/home/ubuntu/users/sjspence/tools/usearch9.2.64_i86linux32'\n",
    "sinaPath = '/home/ubuntu/bin/sina-1.2.11/sina'\n",
    "fasttreePath = '/home/ubuntu/bin/FastTree_dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join paired-end reads\n",
    "subprocess.call([pearPath, '-f', dataDir + '170214Alm_D17-2046_1_sequence.fastq', \n",
    "                 '-r', dataDir + '170214Alm_D17-2046_2_sequence.fastq', '-o', dataDir + '01_pear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71812424\n"
     ]
    }
   ],
   "source": [
    "#Break up file into pieces that usearch can use (5 million lines each)\n",
    "#Run this to completion before running next section\n",
    "inFile = open(dataDir + '01_pear.assembled.fastq', 'r')\n",
    "if not os.path.exists(dataDir + '02_pearSplits/'):\n",
    "    os.makedirs(dataDir + '02_pearSplits/')\n",
    "i = 0\n",
    "j = 1\n",
    "partFile = open(dataDir + '02_pearSplits/pear_' + str(j) + '.fastq', 'w')\n",
    "for line in inFile:\n",
    "    if i >= j*5000000:\n",
    "        partFile.close()\n",
    "        j += 1\n",
    "        partFile = open(dataDir + '02_pearSplits/pear_' + str(j) + '.fastq', 'w')\n",
    "    partFile.write(line)\n",
    "    i += 1\n",
    "partFile.close()\n",
    "inFile.close()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quality filter with usearch 9 max-error rate\n",
    "def qualFilter(inFile, outFile):\n",
    "    subprocess.call([usearchPath, \"-fastq_filter\", inFile, \"-fastq_minlen\", '100', '-fastq_maxee_rate', '0.01',\n",
    "                     \"-fastqout\", outFile], env=env)\n",
    "for filename in os.listdir(dataDir + '02_pearSplits/'):\n",
    "    qualFilter(dataDir + '02_pearSplits/' + filename, dataDir + '02_pearSplits/' + filename.replace('.fastq','filt.fastq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join quality-filtered files back into a single file for processing\n",
    "with open(dataDir + '02_pear_filt.fastq', 'w') as outfile:\n",
    "    for fname in os.listdir(dataDir + '02_pearSplits/'):\n",
    "        if 'filt' in fname:\n",
    "            with open(dataDir + '02_pearSplits/' + fname, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "            infile.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import and edit mapping file\n",
    "sampIDs = []\n",
    "mapping = {}\n",
    "readCounts = {}\n",
    "with open(dataDir + 'OM8_map.txt', 'r') as inFile:\n",
    "    for line in inFile:\n",
    "        if '#' not in line:\n",
    "            line = line.strip().split('\\t')\n",
    "            mapping[line[1]] = line[0].replace('_','s')\n",
    "            readCounts[line[1]] = 0\n",
    "            sampIDs.append(line[0].replace('_','s'))\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Demultiplex: check for barcodes and relabel sequences\n",
    "#Use mapping file to keep barcoded sequences, prepare fasta file\n",
    "with open(dataDir + '02_pear_filt.fastq', 'r') as inFile:\n",
    "    with open(dataDir + '03_pear_filt.fasta', 'w') as outFile:\n",
    "        i = 0\n",
    "        j = 0\n",
    "        nextSeq = False\n",
    "        for line in inFile:\n",
    "            if nextSeq:\n",
    "                outFile.write(line)\n",
    "                nextSeq = False\n",
    "            if i%4 == 0:\n",
    "                for bc in mapping:\n",
    "                    if bc in line:\n",
    "                        readCounts[bc] += 1\n",
    "                        newLine = line.strip().replace('@','>' + mapping[bc] + '_' + str(j) + ' ')\n",
    "                        newLine = newLine + ' orig_bc=' + bc + ' new_bc=' + bc + ' bc_diffs=0\\n'\n",
    "                        outFile.write(newLine)\n",
    "                        nextSeq = True\n",
    "                        j += 1\n",
    "            i += 1\n",
    "inFile.close()\n",
    "outFile.close()\n",
    "#Summarize read mapping after quality filtering and zero-error barcode matching\n",
    "total = 0\n",
    "summaryFile = open(dataDir + '03_quality_summary.txt', 'w')\n",
    "for s in sampIDs:\n",
    "    for bc in mapping:\n",
    "        if mapping[bc] == s:\n",
    "            summaryFile.write(s + '\\t' + str(readCounts[bc]) + '\\n')\n",
    "            total += readCounts[bc]\n",
    "summaryFile.write('Total\\t' + str(total))\n",
    "summaryFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Primer check and removal, placing droplet barcode into header\n",
    "#NOTE: this takes a while\n",
    "qualReads = eb.importFasta(dataDir + '03_pear_filt.fasta')\n",
    "noPrimerReads = eb.filtBarcodePrimers(qualReads, 20, 'GATCATGACCCATTTGGAGAAGATG', 'GGACTACHVGGGTWTCTAAT')\n",
    "eb.exportFasta(noPrimerReads, dataDir + '04_pear_noPrimers.fasta')\n",
    "print(len(qualReads))\n",
    "print(len(noPrimerReads))\n",
    "print(noPrimerReads[0].header)\n",
    "print(noPrimerReads[0].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collapse identical reads and maintain the provenance to reduce the uclust file size\n",
    "#uniqueDict maps a unique sequence to a list of read objects which contain it\n",
    "#NOTE: takes a long time, but rerun after notebook closes out\n",
    "noPrimerReads = eb.importFasta(dataDir + '04_pear_noPrimers.fasta')\n",
    "uniqueDict = eb.getUniqueSeqs(noPrimerReads, dataDir + '05_unique_seqs.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the usearch unoise algorithm to create zero radius OTUs (zOTUs), while also discarding chimeras, phiX sequences,\n",
    "#and low complexity DNA\n",
    "subprocess.call([usearchPath, '-unoise2', dataDir + '05_unique_seqs.fasta', '-fastaout', dataDir + '06_denoised.fa',\n",
    "                 '-otudbout', dataDir + '06_db.fa', '-minampsize', '1'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Unoise output**  \n",
    "00:02 332Mb   100.0% Reading 05_unique_seqs.fasta  \n",
    "01:04 637Mb   100.0% 27163 amplicons, 2096896 bad (size >= 1)  \n",
    "46:06 650Mb   100.0% 14693 good, 12470 chimeras\n",
    "\n",
    "245334 corrected amplicon sequences (including chimeras) in 06_db.fa  \n",
    "14693 output biological sequences in 06_denoised.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#START HERE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4446\n",
      "OM8s01_171 HWI-M04407:1:2105:25283:4847#ACTTATTG/1 orig_bc=ACTTATTG new_bc=ACTTATTG bc_diffs=0 droplet_bc=ATACTCCGTAATGCGAGATC;size=42513;\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary mapping OTU ids to all the headers from unique sequences that map within the OTU\n",
    "otuToHeaders = eb.uniqueSeqsToOTU(dataDir + '06_combined_otus.up')\n",
    "print(len(otuToHeaders.keys()))\n",
    "print(otuToHeaders['OTU2'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write out a file that assigns all non-chimeric read headers to an OTU in a tab-delimited file\n",
    "unique = eb.importFasta(dataDir + '05_unique_seqs.fasta')\n",
    "headerDict = {}\n",
    "for u in unique:\n",
    "    headerDict[u.header.replace('>', '')] = u.seq\n",
    "with open(dataDir + '07_all_seq_otus.txt', 'w') as f:\n",
    "    for otu in otuToHeaders:                  #otus mapped to unique seq headers\n",
    "        for header in otuToHeaders[otu]:      #list of unique seq headers for each otu\n",
    "            seq = headerDict[header]          #get sequence associated with unique seq header\n",
    "            for read in uniqueDict[seq]:      #list of reads associated with a unique sequence\n",
    "                f.write(read.header + '\\t' + otu + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign taxonomy with method=mothur on qiime**\n",
    "\n",
    "* Copy to qiime node, run assign taxonomy with a kmer-based algorithm (RDP or mothur naive Bayes)\n",
    "* RDP needed a taxonomy unavailable from silva, so tried the mothur implementation.\n",
    "\n",
    "$ assign_taxonomy.py -i 06_combined_otus.fa -o 06_taxonomy -m mothur -r HOMD_16S_rRNA_RefSeq_V14.5.p9_spike.fasta -t HOMD_16S_rRNA_RefSeq_V14.5.mothur_spike.taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Within each sample, group by barcode; quantify unique barcode pairings\n",
    "#Need a structure like [sampID1, sampID2, etc.]\n",
    "#Then each sampID maps to a dictionary of droplet barcodes:[otu1, otu2]\n",
    "barcodeSamples = {}\n",
    "with open(dataDir + '07_all_seq_otus.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        samp = line[0].split('_')[0].replace('>','')\n",
    "        bc = line[0].split('droplet_bc=')[1]\n",
    "        otu = line[1]\n",
    "        if samp not in barcodeSamples:\n",
    "            barcodeSamples[samp] = {bc:[otu]}\n",
    "        else:\n",
    "            if bc not in barcodeSamples[samp]:\n",
    "                barcodeSamples[samp][bc] = [otu]\n",
    "            else:\n",
    "                barcodeSamples[samp][bc].append(otu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quantify unique pairings\n",
    "pairDicts = {}\n",
    "for s in sampIDs:\n",
    "    if s in barcodeSamples:\n",
    "        uniquePairs = {}\n",
    "        for bc in barcodeSamples[s]:\n",
    "            if len(barcodeSamples[s][bc]) != 1:\n",
    "                uniqueOTUs = set(barcodeSamples[s][bc])\n",
    "                if len(uniqueOTUs) != 1:\n",
    "                    pairString = '_'.join(list(uniqueOTUs))\n",
    "                    if pairString not in uniquePairs:\n",
    "                        uniquePairs[pairString] = 1\n",
    "                    else:\n",
    "                        uniquePairs[pairString] += 1\n",
    "        pairDicts[s] = uniquePairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import taxonomic information\n",
    "taxDict = eb.importTaxonomy(dataDir + '06_taxonomy/06_combined_otus_tax_assignments.txt', 'mothur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OTU1 is s. onei (#OTU2009, OTU2008)\n",
    "#OTU60 is b. sub...is it anywhere?\n",
    "#OTU1512 is definitely e. coli\n",
    "#OTU2009, OTU1865, OTU3394, OTU2732, OTU989 may be e. coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
